{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Taxi Tip Prediction using Scikit-Learn and Snap ML**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **30** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise session you will consolidate your machine learning (ML) modeling skills by using a popular regression model: Decision Tree. You will use a real dataset to train such a model. The dataset includes information about taxi tip and was collected and provided to the NYC Taxi and Limousine Commission (TLC) by technology providers authorized under the Taxicab & Livery Passenger Enhancement Programs (TPEP/LPEP). You will use the trained model to predict the amount of tip paid. \n",
    "\n",
    "In the current exercise session, you will practice not only the Scikit-Learn Python interface, but also the Python API offered by the Snap Machine Learning (Snap ML) library. Snap ML is a high-performance IBM library for ML modeling. It provides highly-efficient CPU/GPU implementations of linear models and tree-based models. Snap ML not only accelerates ML algorithms through system awareness, but it also offers novel ML algorithms with best-in-class accuracy. For more information, please visit https://www.zurich.ibm.com/snapml/.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing this lab you will be able to:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Perform basic data preprocessing using Scikit-Learn\n",
    "* Model a regression task using the Scikit-Learn and Snap ML Python APIs\n",
    "* Train a Decision Tree Regressor model using Scikit-Learn and Snap ML\n",
    "* Run inference and assess the quality of the trained models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 10px\">\n",
    "    <ol>\n",
    "        <li><a href=\"#introduction\">Introduction</a></li>\n",
    "        <li><a href=\"#import_libraries\">Import Libraries</a></li>\n",
    "        <li><a href=\"#dataset_analysis\">Dataset Analysis</a></li>\n",
    "        <li><a href=\"#dataset_preprocessing\">Dataset Preprocessing</a></li>\n",
    "        <li><a href=\"#dataset_split\">Dataset Train/Test Split</a></li>\n",
    "        <li><a href=\"#dt_sklearn\">Build a Decision Tree Regressor model with Scikit-Learn</a></li>\n",
    "        <li><a href=\"#dt_snap\">Build a Decision Tree Regressor model with Snap ML</a></li>\n",
    "        <li><a href=\"#dt_sklearn_snap\">Evaluate the Scikit-Learn and Snap ML Decision Tree Regressors</a></li>\n",
    "    </ol>\n",
    "</div>\n",
    "<br>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"Introduction\">\n",
    "    <h2>Introduction</h2>\n",
    "    <br>The dataset used in this exercise session is publicly available here: https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page (all rights reserved by Taxi & Limousine Commission(TLC), City of New York). The TLC Yellow Taxi Trip Records of June, 2019 are used in this notebook. The prediction of the tip amount can be modeled as a regression problem. To train the model you can use part of the input dataset and the remaining data can be used to assess the quality of the trained model. First, let's download the dataset.\n",
    "    <br>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-04-01T13:07:55.603403Z"
    }
   },
   "outputs": [],
   "source": [
    "# download June 2020 TLC Yellow Taxi Trip records\n",
    "!curl  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/data/yellow_tripdata_2019-06.csv "
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T13:07:32.854036Z",
     "start_time": "2024-04-01T13:07:32.839953Z"
    }
   },
   "outputs": [],
   "source": [
    "# download June 2020 TLC Yellow Taxi Trip records\n",
    "# Uncomment the next line, if working locally\n",
    "#!curl https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/data/yellow_tripdata_2019-06.csv "
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Did you know?__ When it comes to Machine Learning, you will most likely be working with large datasets. As a business, where can you host your data? IBM is offering a unique opportunity for businesses, with 10 Tb of IBM Cloud Object Storage: [Sign up now for free](http://cocl.us/ML0101EN-IBM-Offer-CC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"import_libraries\">\n",
    "    <h2>Import Libraries</h2>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T13:07:32.838049Z",
     "start_time": "2024-04-01T13:07:20.709911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting snapml==1.8.2\n",
      "  Downloading snapml-1.8.2-cp39-cp39-win_amd64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\anaconda3\\envs\\myenv\\lib\\site-packages (from snapml==1.8.2) (1.23.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\anaconda3\\envs\\myenv\\lib\\site-packages (from snapml==1.8.2) (1.4.1.post1)\n",
      "Requirement already satisfied: scipy in c:\\anaconda3\\envs\\myenv\\lib\\site-packages (from snapml==1.8.2) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\anaconda3\\envs\\myenv\\lib\\site-packages (from scikit-learn->snapml==1.8.2) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\anaconda3\\envs\\myenv\\lib\\site-packages (from scikit-learn->snapml==1.8.2) (3.4.0)\n",
      "Downloading snapml-1.8.2-cp39-cp39-win_amd64.whl (974 kB)\n",
      "   ---------------------------------------- 0.0/974.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 30.7/974.9 kB 1.4 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 92.2/974.9 kB 1.1 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 122.9/974.9 kB 1.0 MB/s eta 0:00:01\n",
      "   ----- -------------------------------- 143.4/974.9 kB 950.9 kB/s eta 0:00:01\n",
      "   --------- ------------------------------ 235.5/974.9 kB 1.1 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 276.5/974.9 kB 1.0 MB/s eta 0:00:01\n",
      "   ----------- -------------------------- 286.7/974.9 kB 886.2 kB/s eta 0:00:01\n",
      "   ----------- -------------------------- 286.7/974.9 kB 886.2 kB/s eta 0:00:01\n",
      "   ------------- ------------------------ 358.4/974.9 kB 857.5 kB/s eta 0:00:01\n",
      "   --------------- ---------------------- 389.1/974.9 kB 836.4 kB/s eta 0:00:01\n",
      "   --------------- ---------------------- 389.1/974.9 kB 836.4 kB/s eta 0:00:01\n",
      "   --------------- ---------------------- 389.1/974.9 kB 836.4 kB/s eta 0:00:01\n",
      "   --------------- ---------------------- 389.1/974.9 kB 836.4 kB/s eta 0:00:01\n",
      "   --------------- ---------------------- 389.1/974.9 kB 836.4 kB/s eta 0:00:01\n",
      "   --------------- ---------------------- 389.1/974.9 kB 836.4 kB/s eta 0:00:01\n",
      "   --------------- ---------------------- 399.4/974.9 kB 566.4 kB/s eta 0:00:02\n",
      "   --------------- ---------------------- 409.6/974.9 kB 511.3 kB/s eta 0:00:02\n",
      "   --------------- ---------------------- 409.6/974.9 kB 511.3 kB/s eta 0:00:02\n",
      "   --------------- ---------------------- 409.6/974.9 kB 511.3 kB/s eta 0:00:02\n",
      "   --------------- ---------------------- 409.6/974.9 kB 511.3 kB/s eta 0:00:02\n",
      "   ---------------- --------------------- 430.1/974.9 kB 440.5 kB/s eta 0:00:02\n",
      "   ----------------- -------------------- 450.6/974.9 kB 433.8 kB/s eta 0:00:02\n",
      "   ----------------- -------------------- 450.6/974.9 kB 433.8 kB/s eta 0:00:02\n",
      "   ----------------- -------------------- 450.6/974.9 kB 433.8 kB/s eta 0:00:02\n",
      "   ----------------- -------------------- 450.6/974.9 kB 433.8 kB/s eta 0:00:02\n",
      "   ----------------- -------------------- 460.8/974.9 kB 379.6 kB/s eta 0:00:02\n",
      "   ----------------- -------------------- 460.8/974.9 kB 379.6 kB/s eta 0:00:02\n",
      "   ------------------ ------------------- 481.3/974.9 kB 372.4 kB/s eta 0:00:02\n",
      "   ------------------ ------------------- 481.3/974.9 kB 372.4 kB/s eta 0:00:02\n",
      "   ------------------ ------------------- 481.3/974.9 kB 372.4 kB/s eta 0:00:02\n",
      "   ------------------- ------------------ 491.5/974.9 kB 338.5 kB/s eta 0:00:02\n",
      "   ------------------- ------------------ 491.5/974.9 kB 338.5 kB/s eta 0:00:02\n",
      "   ------------------- ------------------ 491.5/974.9 kB 338.5 kB/s eta 0:00:02\n",
      "   -------------------- ----------------- 532.5/974.9 kB 324.6 kB/s eta 0:00:02\n",
      "   -------------------- ----------------- 532.5/974.9 kB 324.6 kB/s eta 0:00:02\n",
      "   --------------------- ---------------- 542.7/974.9 kB 318.5 kB/s eta 0:00:02\n",
      "   --------------------- ---------------- 542.7/974.9 kB 318.5 kB/s eta 0:00:02\n",
      "   --------------------- ---------------- 563.2/974.9 kB 310.5 kB/s eta 0:00:02\n",
      "   --------------------- ---------------- 563.2/974.9 kB 310.5 kB/s eta 0:00:02\n",
      "   --------------------- ---------------- 563.2/974.9 kB 310.5 kB/s eta 0:00:02\n",
      "   --------------------- ---------------- 563.2/974.9 kB 310.5 kB/s eta 0:00:02\n",
      "   ---------------------- --------------- 573.4/974.9 kB 286.2 kB/s eta 0:00:02\n",
      "   ----------------------- -------------- 593.9/974.9 kB 287.4 kB/s eta 0:00:02\n",
      "   ----------------------- -------------- 593.9/974.9 kB 287.4 kB/s eta 0:00:02\n",
      "   ----------------------- -------------- 593.9/974.9 kB 287.4 kB/s eta 0:00:02\n",
      "   ----------------------- -------------- 593.9/974.9 kB 287.4 kB/s eta 0:00:02\n",
      "   ----------------------- -------------- 614.4/974.9 kB 272.4 kB/s eta 0:00:02\n",
      "   ----------------------- -------------- 614.4/974.9 kB 272.4 kB/s eta 0:00:02\n",
      "   ------------------------ ------------- 624.6/974.9 kB 263.9 kB/s eta 0:00:02\n",
      "   ------------------------ ------------- 624.6/974.9 kB 263.9 kB/s eta 0:00:02\n",
      "   ------------------------- ------------ 645.1/974.9 kB 263.9 kB/s eta 0:00:02\n",
      "   ------------------------- ------------ 645.1/974.9 kB 263.9 kB/s eta 0:00:02\n",
      "   ------------------------- ------------ 645.1/974.9 kB 263.9 kB/s eta 0:00:02\n",
      "   ------------------------- ------------ 645.1/974.9 kB 263.9 kB/s eta 0:00:02\n",
      "   ------------------------- ------------ 655.4/974.9 kB 248.8 kB/s eta 0:00:02\n",
      "   ------------------------- ------------ 655.4/974.9 kB 248.8 kB/s eta 0:00:02\n",
      "   ------------------------- ------------ 655.4/974.9 kB 248.8 kB/s eta 0:00:02\n",
      "   ------------------------- ------------ 655.4/974.9 kB 248.8 kB/s eta 0:00:02\n",
      "   ------------------------- ------------ 665.6/974.9 kB 237.0 kB/s eta 0:00:02\n",
      "   ------------------------- ------------ 665.6/974.9 kB 237.0 kB/s eta 0:00:02\n",
      "   ------------------------- ------------ 665.6/974.9 kB 237.0 kB/s eta 0:00:02\n",
      "   -------------------------- ----------- 686.1/974.9 kB 231.3 kB/s eta 0:00:02\n",
      "   --------------------------- ---------- 706.6/974.9 kB 233.4 kB/s eta 0:00:02\n",
      "   --------------------------- ---------- 706.6/974.9 kB 233.4 kB/s eta 0:00:02\n",
      "   --------------------------- ---------- 706.6/974.9 kB 233.4 kB/s eta 0:00:02\n",
      "   --------------------------- ---------- 706.6/974.9 kB 233.4 kB/s eta 0:00:02\n",
      "   ---------------------------- --------- 727.0/974.9 kB 226.0 kB/s eta 0:00:02\n",
      "   ---------------------------- --------- 727.0/974.9 kB 226.0 kB/s eta 0:00:02\n",
      "   ---------------------------- --------- 727.0/974.9 kB 226.0 kB/s eta 0:00:02\n",
      "   ---------------------------- --------- 737.3/974.9 kB 220.5 kB/s eta 0:00:02\n",
      "   ---------------------------- --------- 737.3/974.9 kB 220.5 kB/s eta 0:00:02\n",
      "   ----------------------------- -------- 757.8/974.9 kB 221.5 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 757.8/974.9 kB 221.5 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 768.0/974.9 kB 218.5 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 768.0/974.9 kB 218.5 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 768.0/974.9 kB 218.5 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 788.5/974.9 kB 215.6 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 788.5/974.9 kB 215.6 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 788.5/974.9 kB 215.6 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 809.0/974.9 kB 213.0 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 819.2/974.9 kB 210.5 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 819.2/974.9 kB 210.5 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 819.2/974.9 kB 210.5 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 819.2/974.9 kB 210.5 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 839.7/974.9 kB 205.8 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 839.7/974.9 kB 205.8 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 839.7/974.9 kB 205.8 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 839.7/974.9 kB 205.8 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 839.7/974.9 kB 205.8 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 839.7/974.9 kB 205.8 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 849.9/974.9 kB 196.9 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 870.4/974.9 kB 198.7 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 870.4/974.9 kB 198.7 kB/s eta 0:00:01\n",
      "   ---------------------------------- --- 890.9/974.9 kB 199.1 kB/s eta 0:00:01\n",
      "   ---------------------------------- --- 890.9/974.9 kB 199.1 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 901.1/974.9 kB 195.9 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 901.1/974.9 kB 195.9 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 901.1/974.9 kB 195.9 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 901.1/974.9 kB 195.9 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 921.6/974.9 kB 193.2 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 931.8/974.9 kB 192.8 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 931.8/974.9 kB 192.8 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 931.8/974.9 kB 192.8 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 942.1/974.9 kB 190.6 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 942.1/974.9 kB 190.6 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 942.1/974.9 kB 190.6 kB/s eta 0:00:01\n",
      "   -------------------------------------  962.6/974.9 kB 188.1 kB/s eta 0:00:01\n",
      "   -------------------------------------- 974.9/974.9 kB 189.4 kB/s eta 0:00:00\n",
      "Installing collected packages: snapml\n",
      "Successfully installed snapml-1.8.2\n"
     ]
    }
   ],
   "source": [
    "# Snap ML is available on PyPI. To install it simply run the pip command below.\n",
    "!pip install snapml==1.8.2"
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Import the libraries we need to use in this lab\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize, StandardScaler, MinMaxScaler\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "import warnings\n",
    "import gc, sys\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"dataset_analysis\">\n",
    "    <h2>Dataset Analysis</h2>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you will read the dataset in a Pandas dataframe and visualize its content. You will also look at some data statistics.\n",
    "\n",
    "Note: A Pandas dataframe is a two-dimensional, size-mutable, potentially heterogeneous tabular data structure. For more information: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the input data\n",
    "raw_data = pd.read_csv('yellow_tripdata_2019-06.csv')\n",
    "print(\"There are \" + str(len(raw_data)) + \" observations in the dataset.\")\n",
    "print(\"There are \" + str(len(raw_data.columns)) + \" variables in the dataset.\")\n",
    "\n",
    "# display first rows in the dataset\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in the dataset represents a taxi trip. As shown above, each row has 18 variables. One variable is called tip_amount and represents the target variable. Your objective will be to train a model that uses the other variables to predict the value of the tip_amount variable. Let's first clean the dataset and retrieve basic statistics about the target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some trips report 0 tip. it is assumed that these tips were paid in cash.\n",
    "# for this study we drop all these rows\n",
    "raw_data = raw_data[raw_data['tip_amount'] > 0]\n",
    "\n",
    "# we also remove some outliers, namely those where the tip was larger than the fare cost\n",
    "raw_data = raw_data[(raw_data['tip_amount'] <= raw_data['fare_amount'])]\n",
    "\n",
    "# we remove trips with very large fare cost\n",
    "raw_data = raw_data[((raw_data['fare_amount'] >=2) & (raw_data['fare_amount'] < 200))]\n",
    "\n",
    "# we drop variables that include the target variable in it, namely the total_amount\n",
    "clean_data = raw_data.drop(['total_amount'], axis=1)\n",
    "\n",
    "# release memory occupied by raw_data as we do not need it anymore\n",
    "# we are dealing with a large dataset, thus we need to make sure we do not run out of memory\n",
    "del raw_data\n",
    "gc.collect()\n",
    "\n",
    "# print the number of trips left in the dataset\n",
    "print(\"There are \" + str(len(clean_data)) + \" observations in the dataset.\")\n",
    "print(\"There are \" + str(len(clean_data.columns)) + \" variables in the dataset.\")\n",
    "\n",
    "plt.hist(clean_data.tip_amount.values, 16, histtype='bar', facecolor='g')\n",
    "plt.show()\n",
    "\n",
    "print(\"Minimum amount value is \", np.min(clean_data.tip_amount.values))\n",
    "print(\"Maximum amount value is \", np.max(clean_data.tip_amount.values))\n",
    "print(\"90% of the trips have a tip amount less or equal than \", np.percentile(clean_data.tip_amount.values, 90))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display first rows in the dataset\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the dataset in more detail, we see that it contains information such as pick-up and drop-off dates/times, pick-up and drop-off locations, payment types, driver-reported passenger counts etc. Before actually training a ML model, we will need to preprocess the data. We need to transform the data in a format that will be correctly handled by the models. For instance, we need to encode the categorical features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"dataset_preprocessing\">\n",
    "    <h2>Dataset Preprocessing</h2>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this subsection you will prepare the data for training. \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert 'tpep_dropoff_datetime' and 'tpep_pickup_datetime' columns to datetime objects\n",
    "clean_data['tpep_dropoff_datetime'] = pd.to_datetime(clean_data['tpep_dropoff_datetime'])\n",
    "clean_data['tpep_pickup_datetime'] = pd.to_datetime(clean_data['tpep_pickup_datetime'])\n",
    "\n",
    "# Extract pickup and dropoff hour\n",
    "clean_data['pickup_hour'] = clean_data['tpep_pickup_datetime'].dt.hour\n",
    "clean_data['dropoff_hour'] = clean_data['tpep_dropoff_datetime'].dt.hour\n",
    "\n",
    "# Extract pickup and dropoff day of the week (0 = Monday, 6 = Sunday)\n",
    "clean_data['pickup_day'] = clean_data['tpep_pickup_datetime'].dt.weekday\n",
    "clean_data['dropoff_day'] = clean_data['tpep_dropoff_datetime'].dt.weekday\n",
    "\n",
    "# Calculate trip time in seconds\n",
    "clean_data['trip_time'] = (clean_data['tpep_dropoff_datetime'] - clean_data['tpep_pickup_datetime']).dt.total_seconds()\n",
    "\n",
    "# Ideally use the full dataset for this exercise.\n",
    "# However, if you run into out-of-memory issues due to the data size, reduce it.\n",
    "# For instance, in this example, we use only the first 200,000 samples.\n",
    "first_n_rows = 200000\n",
    "clean_data = clean_data.head(first_n_rows)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the pickup and dropoff datetimes\n",
    "clean_data = clean_data.drop(['tpep_pickup_datetime', 'tpep_dropoff_datetime'], axis=1)\n",
    "\n",
    "# some features are categorical, we need to encode them\n",
    "# to encode them we use one-hot encoding from the Pandas package\n",
    "get_dummy_col = [\"VendorID\",\"RatecodeID\",\"store_and_fwd_flag\",\"PULocationID\", \"DOLocationID\",\"payment_type\", \"pickup_hour\", \"dropoff_hour\", \"pickup_day\", \"dropoff_day\"]\n",
    "proc_data = pd.get_dummies(clean_data, columns = get_dummy_col)\n",
    "\n",
    "# release memory occupied by clean_data as we do not need it anymore\n",
    "# we are dealing with a large dataset, thus we need to make sure we do not run out of memory\n",
    "del clean_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the labels from the dataframe\n",
    "y = proc_data[['tip_amount']].values.astype('float32')\n",
    "\n",
    "# drop the target variable from the feature matrix\n",
    "proc_data = proc_data.drop(['tip_amount'], axis=1)\n",
    "\n",
    "# get the feature matrix used for training\n",
    "X = proc_data.values\n",
    "\n",
    "# normalize the feature matrix\n",
    "X = normalize(X, axis=1, norm='l1', copy=False)\n",
    "\n",
    "# print the shape of the features matrix and the labels vector\n",
    "print('X.shape=', X.shape, 'y.shape=', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"dataset_split\">\n",
    "    <h2>Dataset Train/Test Split</h2>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dataset is ready for building the classification models, you need to first divide the pre-processed dataset into a subset to be used for training the model (the train set) and a subset to be used for evaluating the quality of the model (the test set).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "print('X_train.shape=', X_train.shape, 'Y_train.shape=', y_train.shape)\n",
    "print('X_test.shape=', X_test.shape, 'Y_test.shape=', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"dt_sklearn\">\n",
    "    <h2>Build a Decision Tree Regressor model with Scikit-Learn</h2>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the Decision Tree Regression Model from scikit-learn\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# for reproducible output across multiple function calls, set random_state to a given integer value\n",
    "sklearn_dt = DecisionTreeRegressor(max_depth=8, random_state=35)\n",
    "\n",
    "# train a Decision Tree Regressor using scikit-learn\n",
    "t0 = time.time()\n",
    "sklearn_dt.fit(X_train, y_train)\n",
    "sklearn_time = time.time()-t0\n",
    "print(\"[Scikit-Learn] Training time (s):  {0:.5f}\".format(sklearn_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"dt_snapml\">\n",
    "    <h2>Build a Decision Tree Regressor model with Snap ML</h2>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the Decision Tree Regressor Model from Snap ML\n",
    "from snapml import DecisionTreeRegressor\n",
    "\n",
    "# in contrast to sklearn's Decision Tree, Snap ML offers multi-threaded CPU/GPU training \n",
    "# to use the GPU, one needs to set the use_gpu parameter to True\n",
    "# snapml_dt = DecisionTreeRegressor(max_depth=4, random_state=45, use_gpu=True)\n",
    "\n",
    "# to set the number of CPU threads used at training time, one needs to set the n_jobs parameter\n",
    "# for reproducible output across multiple function calls, set random_state to a given integer value\n",
    "snapml_dt = DecisionTreeRegressor(max_depth=8, random_state=45, n_jobs=4)\n",
    "\n",
    "# train a Decision Tree Regressor model using Snap ML\n",
    "t0 = time.time()\n",
    "snapml_dt.fit(X_train, y_train)\n",
    "snapml_time = time.time()-t0\n",
    "print(\"[Snap ML] Training time (s):  {0:.5f}\".format(snapml_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"dt_sklearn_snapml\">\n",
    "    <h2>Evaluate the Scikit-Learn and Snap ML Decision Tree Regressor Models</h2>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snap ML vs Scikit-Learn training speedup\n",
    "training_speedup = sklearn_time/snapml_time\n",
    "print('[Decision Tree Regressor] Snap ML vs. Scikit-Learn speedup : {0:.2f}x '.format(training_speedup))\n",
    "\n",
    "# run inference using the sklearn model\n",
    "sklearn_pred = sklearn_dt.predict(X_test)\n",
    "\n",
    "# evaluate mean squared error on the test dataset\n",
    "sklearn_mse = mean_squared_error(y_test, sklearn_pred)\n",
    "print('[Scikit-Learn] MSE score : {0:.3f}'.format(sklearn_mse))\n",
    "\n",
    "# run inference using the Snap ML model\n",
    "snapml_pred = snapml_dt.predict(X_test)\n",
    "\n",
    "# evaluate mean squared error on the test dataset\n",
    "snapml_mse = mean_squared_error(y_test, snapml_pred)\n",
    "print('[Snap ML] MSE score : {0:.3f}'.format(snapml_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above both decision tree models provide the same score on the test dataset. However Snap ML runs the training routine faster than Scikit-Learn. This is one of the advantages of using Snap ML: acceleration of training of classical machine learning models, such as linear and tree-based models. For more Snap ML examples, please visit https://github.com/IBM/snapml-examples. Moreover, as shown above, not only is Snap ML seemlessly accelerating scikit-learn applications, but the library's Python API is also compatible with scikit-learn metrics and data preprocessors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets train a `SnapML` `Decision Tree Regressor` with the `max_depth` parameter set to `12`, `random_state` set to `45`, and `n_jobs` set to `4` and compare its Mean Squared Error to the decision tree regressor we trained previously\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by creating and training the decision tree\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    "\n",
    "```python    \n",
    "tree = DecisionTreeRegressor(max_depth=12, random_state=45, n_jobs=4)\n",
    "\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate the Mean Squared Error on the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    "\n",
    "```python    \n",
    "pred = tree.predict(X_test)\n",
    "\n",
    "print(\"MSE: \", mean_squared_error(y_test, pred))\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learned that increasing the `max_depth` parameter to `12` increases the MSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andreea Anghel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Contributors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sangeeth Keeriyadath \n",
    "\n",
    "Joseph Santarcangelo\n",
    "\n",
    "Azim Hirjani\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2021-08-31  | 0.1  | AAN  |  Created Lab Content |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Copyright &copy; 2021 IBM Corporation. This notebook and its source code are released under the terms of the [MIT License](https://cognitiveclass.ai/mit-license/).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
